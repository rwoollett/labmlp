    //    MatrixXd nHiddenActivationWithBias(m_nData, m_nHidden + 1);
//    MatrixXd outputs(m_nData, m_nOut);
    // nHiddenActivationWithBias.block(0, 0, m_nData, m_nHidden) << inputsWithBiasEntry * m_weights1;
    // nHiddenActivationWithBias.col(m_nHidden).tail(m_nData) << biasInput;
    // the following equationa are the fwd process of the mlp train
    // TODO: make the fwd process just do the fwd step and a new bckpropagate function in the train
    // for (int nData = 0; nData < m_nData; nData++)
    // {
    //   for (int n = 0; n < m_nHidden; n++)
    //   {
    //     double resH = (1.0 / (1.0 + std::exp(-1.0 * m_beta * nHiddenActivationWithBias(nData, n))));
    //     nHiddenActivationWithBias(nData, n) = resH; //(1.0 / (1.0 + std::exp(-1.0 * m_beta * nHiddenActivationWithBias(nData, n))));
    //   }
    // }

    
    // outputs = nHiddenActivationWithBias * m_weights2;

    // for (int nData = 0; nData < m_nData; nData++)
    // {
    //   for (int o = 0; o < m_nOut; o++)
    //   {
    //     double resO = 1.0 / (1.0 + std::exp(-1.0 * m_beta * outputs(nData, o)));
    //     outputs(nData, o) = resO;
    //   }
    // }
    // end of fwd process - outputs are the activated results


>>> train
array([[0.        ],
       [0.05128205],
       [0.1025641 ],
       [0.15384615],
       [0.20512821],
       [0.25641026],
       [0.30769231],
       [0.35897436],
       [0.41025641],
       [0.46153846],
       [0.51282051],
       [0.56410256],
       [0.61538462],
       [0.66666667],
       [0.71794872],
       [0.76923077],
       [0.82051282],
       [0.87179487],
       [0.92307692],
       [0.97435897]])
>>> traintarget
array([[ 1.15630447],
       [ 0.82713399],
       [ 0.63608309],
       [ 0.63571089],
       [ 0.05677841],
       [-0.05858336],
       [ 0.22443582],
       [ 0.21078908],
       [ 0.99071272],
       [ 0.94617025],
       [ 0.94200553],
       [ 0.6522204 ],
       [-0.1306565 ],
       [-1.41860284],
       [-1.80732162],
       [-2.03117594],
       [-1.09663447],
       [-0.76640559],
       [-0.28514105],
       [ 0.52244289]])
>>> test
array([[0.02564103],
       [0.12820513],
       [0.23076923],
       [0.33333333],
       [0.43589744],
       [0.53846154],
       [0.64102564],
       [0.74358974],
       [0.84615385],
       [0.94871795]])
>>> testtarget
array([[ 1.25139796],
       [ 0.45932739],
       [ 0.22442944],
       [ 0.29171431],
       [ 1.35242154],
       [ 0.84426476],
       [-1.04324391],
       [-2.07895302],
       [-1.11569002],
       [ 0.4691491 ]])
>>> valid
array([[0.07692308],
       [0.17948718],
       [0.28205128],
       [0.38461538],
       [0.48717949],
       [0.58974359],
       [0.69230769],
       [0.79487179],
       [0.8974359 ],
       [1.        ]])
>>> validtarget
array([[ 0.70175258],
       [ 0.08750527],
       [ 0.05474888],
       [ 0.46319713],
       [ 1.25815812],
       [ 0.09187942],
       [-1.31405965],
       [-1.67965025],
       [-0.49185788],
       [ 1.09831111]])
>>> 



-0.361426   0.331352  -0.750849  -0.767418          0
 -0.458671 -0.0401189  -0.750849  -0.767418          0
 -0.555916    0.10847  -0.782683  -0.767418          0
 -0.604538  0.0341753  -0.719015  -0.767418          0
 -0.410049   0.405646  -0.750849  -0.767418          0

 [[-0.36142626  0.33135215 -0.7508489  -0.76741803  0.        ]
 [-0.45867099 -0.04011887 -0.7508489  -0.76741803  0.        ]
 [-0.55591572  0.10846954 -0.78268251 -0.76741803  0.        ]
 [-0.60453809  0.03417533 -0.71901528 -0.76741803  0.        ]
 [-0.41004862  0.40564636 -0.7508489  -0.76741803  0.        ]]